{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance of multi-layer NN and CNN on Fashion-MNIST dataset \n",
    "\n",
    "\n",
    "#### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading fashion-MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\fashion\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\fashion\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "fashion = input_data.read_data_sets('data\\\\fashion', one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shapes of different dataset partition\n",
    "dataset size: images and correspodning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training image set: (55000, 784) | train label set: (55000, 10)\n",
      "shape of test image set: (10000, 784) | test label set: (55000, 10)\n"
     ]
    }
   ],
   "source": [
    "#dataset size: images and correspodning labels\n",
    "print('shape of training image set: {train} | train label set: {train_label}'.format(train= fashion.train.images.shape, train_label= fashion.train.labels.shape))\n",
    "print('shape of test image set: {test} | test label set: {test_label}'.format(test= fashion.test.images.shape, test_label= fashion.train.labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining image labeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\n"
     ]
    }
   ],
   "source": [
    "targets= dict(zip([0,1,2,3,4,5,6,7,8,9],'T-shirt/top,Trouser,Pullover,Dress,Coat,Sandal,Shirt,Sneaker,Bag,Ankle boot'.split(',')))\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing a sample image and label corresponding to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a070315940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(fashion.train.images[1].reshape(28,28))\n",
    "plt.title('image label: '+targets[np.where(fashion.train.labels[1]==1)[0][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining placholders for storing image data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ease of definition functions for defining Weights and Bias parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights(name, shape):\n",
    "    #with tf.variable_scope(\"weights\"):#, reuse=True):\n",
    "    ini = tf.get_variable(name, shape, initializer= tf.contrib.layers.xavier_initializer())#tf.truncated_normal(shape,stddev=0.1)\n",
    "    return ini\n",
    "def bias(name, shape):\n",
    "    #with tf.variable_scope(\"biases\"):#, reuse=True):\n",
    "    ini = tf.get_variable(name, shape, initializer= tf.zeros_initializer())\n",
    "    return ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for defining structure of Neural Network\n",
    "this function is also responsible for perform forward propagation during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after correction\n",
    "def network_layers(X, neuron_count): # ex: units_per_layer = [784,128,64,28]\n",
    "    #x = tf.placeholder(tf.float32, shape=[None, neuron_count[0]]) #neuron_count[0]= no. of neurons in input layer = num of image pixels i.e. 784\n",
    "    #y_true = tf.placeholder(tf.float32, shape=[None, neuron_count[-1]]) #neuron_count[-1]= no. of neurons in output layer = total no. of labels i.e. 10\n",
    "    \n",
    "    hidden_layer_count = len(neuron_count) - 2 # intermediate layers excluding input and output layers neuron_count[0] &[-1]\n",
    "    units_per_layer= neuron_count[1:] #units_per_layer = all layers except input i.e., [128,64,28]   \n",
    "    layers = [X] #layers, a list that holds each network layer, initially just input layer[0]= x, shaped [None, 784]\n",
    "    params = [] \n",
    "    b_params = [] #b_params, list holding each network layer's bias parameter, 1st vector shaped [1 x neurons in units_per_layer[0]] i.e. [1,128]\n",
    "    print('Number of hidden layers defined: ', hidden_layer_count)\n",
    "    print('Number of Neurons per layer: ', units_per_layer)\n",
    "    print('no. parameter matrices & bias vectors added to list prior to network definition: ',len(params),',',len(b_params),'\\n pre-network definition block 1: done \\n')\n",
    "    \n",
    "    for num in range(hidden_layer_count): #will loop over twice for 2 hidden layer of size 128 and 64 neurons\n",
    "        d= [int(layers[num].shape[1]), units_per_layer[num]] #d = shape vector correspoding params for each layer, @ num= 0, d= [784,128]\n",
    "        params.append(weights('weights'+str(num+1), d))#params[0] shaped [784,128] corresponds to layer 1\n",
    "        print('param: ',num+1, ' is named: ',params[num])\n",
    "        \n",
    "        e= [1, units_per_layer[num]] #e = shape vector correspoding to bias for each layer, e= [1,128]\n",
    "        b_params.append(bias('biases'+str(num+1), e)) #b_params[0] shaped [1,128] corresponds to layer 1\n",
    "        print('bias: ',num+1, 'is named: ',b_params[num])\n",
    "        \n",
    "        #layer[0] shaped [None,784], params[0] shaped [784,28], biase[0] shaped [1,28]\n",
    "        weighted_sum = tf.matmul(layers[num], params[num]) + b_params[num] #matmul op shaped [none,128] for 1st hidden layer\n",
    "        layers.append(tf.nn.relu(weighted_sum)) #layer[1] is shaped [None, 128]\n",
    "        \n",
    "        print('layer: {_} is shaped : {l_shape}'.format(_= num+1, l_shape= layers[num+1].shape))\n",
    "        print('params {_} is shaped: {p_shape} & bias_params {_} is shaped: {b_shape} \\n'.format(_= num+1, p_shape= params[num].shape,b_shape= b_params[num].shape))\n",
    "        \n",
    "    params.append(weights('final_weights', [int(layers[-1].shape[1]), units_per_layer[-1]]))#appeding final predition layer weights \n",
    "    b_params.append(bias('final_bias', [1, units_per_layer[-1]]))\n",
    "    \n",
    "    prediction_layer = tf.matmul(layers[-1], params[-1]) + b_params[-1] #weights('final_weights', [int(layer[-1].shape[1]), units_per_layer[-1]])) + bias('final_bias', [1, units_per_layer[-1]])\n",
    "    layers.append(prediction_layer)    \n",
    "    print('final layer is shaped: {l_shape}'.format(l_shape= layers[-1].shape))\n",
    "    print('final weight is shaped: {p_shape} & bias_params is shaped: {b_shape} \\n'.format(p_shape= params[-1].shape,b_shape= b_params[-1].shape))\n",
    "    print('layers list size: ', len(layers), '| params list size: ',len(params),'| bias list size:', len(b_params),'\\n')\n",
    "    \n",
    "    print('Zipping params and Bias_params into a list of tuples as \"Parameters\". . .')\n",
    "    parameters = zip(params, b_params) # ex- [(weights1, bias1), (weights2, bias2)]\n",
    "\n",
    "\n",
    "    return [layers, parameters]#, prediction_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#a,b = network_layers(x,[784,128,64,10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A final model function\n",
    "###### that calls parameters from subordinate function during forward pass of training data and Backpropagating errors to perform parameters(Weights and bias) updation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_data,test_data, neuron_count , learn_rate, epoch_count, batch_size):\n",
    "    tf.reset_default_graph()\n",
    "    x= tf.placeholder(tf.float32, shape=[None, neuron_count[0]]) #train_data\n",
    "    y_true= tf.placeholder(tf.float32, shape=[None, neuron_count[-1]])#train_label    \n",
    "    \n",
    "    #forward passing train data\n",
    "    #all_layers = network_layers(neuron_count)[0]\n",
    "    layers, parameters= network_layers(x ,neuron_count) # network_layers returns [layers, parameters]\n",
    "    y_prediction = layers[-1] #prediction_layer is last element of returned layers listi.e. layers[-1]\n",
    "    pred_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_prediction,1), tf.argmax(y_true,1)), tf.int32))\n",
    "    #prediction loss & thereafter backpropagating the loss through weight optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_true, logits= y_prediction))\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "    \n",
    "    init= tf.global_variables_initializer() \n",
    "                              #initilize all variables\n",
    "    \n",
    "    #for testing this funtion on small data\n",
    "    num_batches = 10 # (x.shape[0])/(batch_size) # (total no. of images)/(no.of images in a batch)\n",
    "    \n",
    "    acc_timeline= []\n",
    "    cost_timeline = [] #storing the spectrum of loss after every epoch\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for num in range(epoch_count):\n",
    "            epoch_cost=0\n",
    "            for i in range(num_batches):\n",
    "                batchX, batchY = train_data.next_batch(batch_size)#train_image.next_batch(batch_size),train_label.next_batch(batch_size) #, train_label.next_batch(batch_size)]            \n",
    "                temp, batch_cost,acc  = sess.run([optimizer, cost, pred_accuracy] ,{x:batchX, y_true:batchY})\n",
    "                epoch_cost+= batch_cost/num_batches\n",
    "            print('Cost at epoch {epoch}, train accuracy: {acc}'.format(epoch= num, acc= acc))\n",
    "            acc_timeline.append(acc) #appending accuracy value after every epoch\n",
    "            cost_timeline.append(epoch_cost) #appending cost value after every epoch\n",
    "                        \n",
    "        print('All parameters have been learned! \\n')\n",
    "        print('Gross training accuracy on dataset of {size} is: {acc}'.format(size= train_data.images.shape[0],\n",
    "                                                        acc= pred_accuracy.eval({x:train_data.images, y_true: train_data.labels})))\n",
    "        print('Gross training accuracy on dataset of {size} is: {acc}'.format(size= test_data.images.shape[0],\n",
    "                                                        acc= pred_accuracy.eval({x:test_data.images, y_true: test_data.labels})))\n",
    "    \n",
    "    #a= raw_input('type: \"y\" ,to return the learned parameters or \"n\" to exit')\n",
    "    #if(a== 'Y'):\n",
    "        \n",
    "          \n",
    "        \n",
    "    \n",
    "    #plt.figure(figsize=(16,5))\n",
    "    #plt.plot(np.squeeze(cost_timeline))#, color='#2A688B')\n",
    "    #plt.xlim(0, num_epochs-1)\n",
    "    #plt.ylabel(\"prediction_cost\")\n",
    "    #plt.xlabel(\"iterations\")\n",
    "    #plt.title(\"Cost curve at learning rate = {rate}\".format(rate=learn_rate))\n",
    "    #plt.savefig(graph_filename, dpi=300)\n",
    "    #plt.show()\n",
    "    \n",
    "    #plt.figure(figsize=(16,5))\n",
    "    #plt.plot(np.squeeze(acc_timeline))#, color='#2A688B')\n",
    "    #plt.xlim(0, num_epochs-1)\n",
    "    #plt.ylabel(\"prediction_cost\")\n",
    "    #plt.xlabel(\"epochs\")\n",
    "    #plt.title(\"Accuracy curve at learning rate = {rate}\".format(rate=learn_rate))\n",
    "    #plt.savefig(graph_filename, dpi=300)\n",
    "    #plt.show()\n",
    "    \n",
    "        parameters = sess.run(parameters)\n",
    "    \n",
    "        return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden layers defined:  2\n",
      "Number of Neurons per layer:  [64, 28, 10]\n",
      "no. parameter matrices & bias vectors added to list prior to network definition:  0 , 0 \n",
      " pre-network definition block 1: done \n",
      "\n",
      "param:  1  is named:  <tf.Variable 'weights1:0' shape=(784, 64) dtype=float32_ref>\n",
      "bias:  1 is named:  <tf.Variable 'biases1:0' shape=(1, 64) dtype=float32_ref>\n",
      "layer: 1 is shaped : (?, 64)\n",
      "params 1 is shaped: (784, 64) & bias_params 1 is shaped: (1, 64) \n",
      "\n",
      "param:  2  is named:  <tf.Variable 'weights2:0' shape=(64, 28) dtype=float32_ref>\n",
      "bias:  2 is named:  <tf.Variable 'biases2:0' shape=(1, 28) dtype=float32_ref>\n",
      "layer: 2 is shaped : (?, 28)\n",
      "params 2 is shaped: (64, 28) & bias_params 2 is shaped: (1, 28) \n",
      "\n",
      "final layer is shaped: (?, 10)\n",
      "final weight is shaped: (28, 10) & bias_params is shaped: (1, 10) \n",
      "\n",
      "layers list size:  4 | params list size:  3 | bias list size: 3 \n",
      "\n",
      "Zipping params and Bias_params into a list of tuples as \"Parameters\". . .\n",
      "Cost at epoch 0, train accuracy: 0\n",
      "Cost at epoch 1, train accuracy: 0\n",
      "All parameters have been learned! \n",
      "\n",
      "Gross training accuracy on dataset of 55000 is: 0\n",
      "Gross training accuracy on dataset of 10000 is: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument <zip object at 0x000002A07049BC48> has invalid type <class 'zip'>, must be a string or Tensor. (Can not convert a zip into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    269\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 270\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    271\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2707\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2708\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   2796\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[1;32m-> 2797\u001b[1;33m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[0;32m   2798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a zip into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bbaa949e5fa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfashion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfashion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-4589227978a1>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(train_data, test_data, neuron_count, learn_rate, epoch_count, batch_size)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m#plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1109\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \"\"\"\n\u001b[0;32m    412\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    272\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    273\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    275\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument <zip object at 0x000002A07049BC48> has invalid type <class 'zip'>, must be a string or Tensor. (Can not convert a zip into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "p1= model(fashion.train,fashion.test,[784,64,28,10],0.0001, 2, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating fashion-dataset on CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ease-Function for defining Weights, biases, convolution and pooling layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_weights(shape):\n",
    "    ini = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(ini)\n",
    "def c_bias(shape):\n",
    "    ini = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our CNN model: \n",
    " ##### 1. setting weights & bias for CNN-1;\n",
    " ##### 2. computing the outputs for CNN-1\n",
    " ##### 3. computing the outputs for Pool-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "w_conv1 = c_weights([5,5,1,32]) #the number of channels formed in 1st conv layer will be 32, each size 14x14 after pool1\n",
    "b_conv1= c_bias([32])\n",
    "print(w_conv1, b_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Reshape_1:0\", shape=(?, 28, 28, 1), dtype=float32) must be from the same graph as Tensor(\"Variable:0\", shape=(5, 5, 1, 32), dtype=float32_ref).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-87ea4bfc1a36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh_conv1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mb_conv1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#shape 28x28x32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpool_1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_conv1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#shape 14x14x32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-e291eb526fda>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, W)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    398\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    328\u001b[0m       \u001b[1;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   4260\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4261\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4262\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4263\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4264\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mc:\\python3.5.3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   4199\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4200\u001b[0m     raise ValueError(\n\u001b[1;32m-> 4201\u001b[1;33m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[0;32m   4202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"Reshape_1:0\", shape=(?, 28, 28, 1), dtype=float32) must be from the same graph as Tensor(\"Variable:0\", shape=(5, 5, 1, 32), dtype=float32_ref)."
     ]
    }
   ],
   "source": [
    "h_conv1= tf.nn.relu(conv2d(x_image,w_conv1)+ b_conv1) #shape 28x28x32\n",
    "pool_1= pool(h_conv1) #shape 14x14x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(28), Dimension(28), Dimension(32)])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv1.shape # 1st dim is the number of images in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 1. setting weights & bias for CNN-2;\n",
    " ##### 2. computing the outputs for CNN-2;\n",
    " ##### 3. computing the outputs for Pool-2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_conv2 = c_weights([5,5,32,64]) #will produce 64 channels in 2nd conv layer, w_conv2 shape [5x5x32, 64]\n",
    "b_conv2 = c_bias([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv2 = tf.nn.relu(conv2d(pool_1, w_conv2)+ b_conv2) #here input is pool_1. shape 14x14x32, shape of 1 of 64 kernel applied is [5x5x32]h_conv2 will have shape 14x14x64\n",
    "pool_2 = pool(h_conv2) #2nd pooling layer, output shape 7x7x64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(5, 5, 32, 64) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref> Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32) Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(w_conv2, b_conv2, h_conv2, pool_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 1. setting weights & bias for densely connected layer 1;\n",
    " ##### 2. reshaping [7,7,64] shaped tensor to a flattened [7x 7x64] dimenional vector;\n",
    " ##### 3. computing the outputs for fc-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight & bias shapes for fc layer <tf.Variable 'Variable_6:0' shape=(3136, 1024) dtype=float32_ref> <tf.Variable 'Variable_7:0' shape=(1024,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "w_fc1 = c_weights([7*7*64, 1024]) #weights for final layer, weight shape [7*7*64 x 1024]\n",
    "b_fc1= c_bias([1024])\n",
    "print('Weight & bias shapes for fc layer', w_fc1, b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_fc_flat = tf.reshape(pool_2, [-1, 7*7*64]) #pool_2 layer had been flattened to 7*7*64 vector for one image\n",
    "h_fc1 = tf.nn.relu(tf.matmul(pool_fc_flat, w_fc1)+ b_fc1) #matmul of pool2_flat(shaped[1, 7*7*64]) and \n",
    "                                           #w_fc1(shaped[7*7*64, 1024] yields a  result matrix h_fc1 shaped [1,1024] for each image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool fc flat & h_fc1 shape: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " TensorShape([Dimension(None), Dimension(3136)]),\n",
       " TensorShape([Dimension(None), Dimension(1024)]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('pool fc flat & h_fc1 shape: '), pool_fc_flat.shape, h_fc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_fc1 & b_fc1 shape: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " TensorShape([Dimension(3136), Dimension(1024)]),\n",
       " TensorShape([Dimension(1024)]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('w_fc1 & b_fc1 shape: '), w_fc1.shape, b_fc1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 1. performing dropout on output of previous layer(fc1) to prevent overfitting on training dataset;\n",
    " ##### 2. setting weights & bias for final fully connected prediction layer;\n",
    " ##### 3. computing the outputs for y_pred by mapping outputs of fc1 to fc2  by performing at weighted sum of fc1 output shaped [1,1024] and\n",
    "#####       fc2weights shaped [1024,10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) #adding a dropout to h_fc1 layer shaped [1, 1024] for each image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fc2 = c_weights([1024,10]) #weight matrix to map h_Fc1 shaped [1,1024] to h_Fc2 shaped [1,10]\n",
    "b_fc2 = c_bias([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.matmul(h_fc1_drop, w_fc2) + b_fc2 # prediction vector layer y_pred shaped [1,10] for 1 input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training layers & updating wieghts & biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy after  0 images is: 0.15\n",
      "train accuracy after  200 images is: 0.15\n",
      "train accuracy after  400 images is: 0.35\n",
      "train accuracy after  600 images is: 0.5\n",
      "train accuracy after  800 images is: 0.55\n",
      "train accuracy after  1000 images is: 0.5\n",
      "train accuracy after  1200 images is: 0.95\n",
      "train accuracy after  1400 images is: 0.6\n",
      "train accuracy after  1600 images is: 0.65\n",
      "train accuracy after  1800 images is: 1.0\n",
      "train accuracy after  2000 images is: 0.65\n",
      "train accuracy after  2200 images is: 0.85\n",
      "train accuracy after  2400 images is: 0.8\n",
      "train accuracy after  2600 images is: 0.8\n",
      "test data accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(140):\n",
    "        batch = fashion.train.next_batch(20)\n",
    "        if i%10 ==0:\n",
    "            train_acc = accuracy.eval({x:batch[0], y_:batch[1], keep_prob:1.0})\n",
    "            print('train accuracy after ',i*20, 'images is:', train_acc)\n",
    "        train_step.run({x:batch[0], y_:batch[1], keep_prob:0.5})\n",
    "    print('test data accuracy', accuracy.eval({x: fashion.test.images[1:21], y_:fashion.test.labels[1:21], keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.099502489"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval({x:mnist.train.images[2400:2601], y_:mnist.train.labels[2400:2601], keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable_6:0' shape=(3136, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_8:0' shape=(1024, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_9:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Tensor 'add_4:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_fc1, w_fc2, b_fc2, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.31374872  3.30314469  3.84198737 -2.11247444  4.78950644  0.19314094\n",
      "   4.47027588 -1.2290597   1.04318416 -5.097332  ]\n",
      " [ 0.95248878  0.8565222   2.70032001 -0.57708746  3.56639004  3.16016126\n",
      "   1.06598711 -2.27347255  2.6269505  -7.79926586]]\n",
      "random datapoints predictions vs. label match is found to be :  [False False] \n",
      "max probability positions in y_pred and y_ are:  [4 4] [7 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.eval({x:fashion.train.images[1:3],keep_prob:1.0}))\n",
    "print('random datapoints predictions vs. label match is found to be : ', correct_prediction.eval({x:fashion.train.images[1:3],\n",
    "y_:fashion.train.labels[1:3],keep_prob:1.0}),'\\nmax probability positions in y_pred and y_ are: ' ,\n",
    "tf.argmax(y_pred,1).eval({x:fashion.train.images[1:3], keep_prob:1.0}), tf.argmax(fashion.train.labels[1:3],1).eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
